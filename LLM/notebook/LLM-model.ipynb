{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "355a88e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers accelerate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a44318e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "058bfe50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92d89433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9e4da7f2584c03af693a3fa89f61fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "01bf2ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1724)\n",
      "What\n",
      "tensor(338)\n",
      "is\n",
      "tensor(26163)\n",
      "apple\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is apple\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "for id in input_ids[0]:\n",
    "    print(id)\n",
    "    print(tokenizer.decode(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f268ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model.model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "aa9c312c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3072])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b6a5777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_head_output = model.lm_head(model_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7a02c7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32064])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_head_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "efffd7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(338)\n",
      "is\n",
      "tensor(278)\n",
      "the\n",
      "tensor(3623)\n",
      "ju\n"
     ]
    }
   ],
   "source": [
    "for token_vector in lm_head_output[0]:\n",
    "    token_id = token_vector.argmax(-1)\n",
    "    print(token_id)\n",
    "    print(tokenizer.decode(token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7c1a77cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'ju'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_id = lm_head_output[0, -1].argmax(-1)\n",
    "tokenizer.decode(token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96116db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False, # False means to not include the prompt text in the returned text\n",
    "    max_new_tokens=50, \n",
    "    do_sample=False, # no randomness in the generated text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67edc3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/.cache/huggingface\n",
      "['.locks', 'models--microsoft--Phi-3-mini-4k-instruct']\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub.constants import HF_HOME\n",
    "print(HF_HOME)\n",
    "import os\n",
    "\n",
    "print(os.listdir(f\"{HF_HOME}/hub\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "78608edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " juice? Apple juice is a beverage made from the liquid extracted from apples. It can be consumed as a refreshing drink or used as an ingredient in various culinary applications. Apple juice is typically made by\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"Explain decoder self attention in a concise manner.\"\n",
    "\n",
    "output = generator(prompt)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af6a0441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm()\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10f61724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3DecoderLayer(\n",
       "  (self_attn): Phi3Attention(\n",
       "    (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "    (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "  )\n",
       "  (mlp): Phi3MLP(\n",
       "    (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "    (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "    (activation_fn): SiLUActivation()\n",
       "  )\n",
       "  (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "  (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "  (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
