{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e4662f6-0865-4456-84ad-a1059f6e30bd",
   "metadata": {},
   "source": [
    "### encoder-decoder-attention\n",
    "K,V from encoder\n",
    "Q from decoder\n",
    "\n",
    "use in multi-model models (vision and text)\n",
    "encoder that has been trained on images with image tokens \n",
    "its context aware embeddings can be fed into text based decoder via cross attention to generate image with captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54665602-9a9b-4d33-bbda-521c33eafa3c",
   "metadata": {},
   "source": [
    "### multi head attention\n",
    "multiple heads to learn different semantic meanings\n",
    "\n",
    "assuming each head has 2 dimension value with 3 heads\n",
    "2x3 -> 6 attention values \n",
    "\n",
    "we can set the model dimension to 6 (model dimension/num of heads)\n",
    "\n",
    "but you can also use feed forward layer to use matrix multiplication to get back to the correct model dimension to go from 6 -> 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb86115f-6621-4095-99e1-263e28b89046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch ## torch let's us create tensors and also provides helper functions\n",
    "import torch.nn as nn ## torch.nn gives us nn.module() and nn.Linear()\n",
    "import torch.nn.functional as F # This gives us the softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25f28d34-58a6-47f5-954d-00ae16faefe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module): \n",
    "                            \n",
    "    def __init__(self, d_model=2,  \n",
    "                 row_dim=0, \n",
    "                 col_dim=1):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        \n",
    "        self.row_dim = row_dim\n",
    "        self.col_dim = col_dim\n",
    "\n",
    "\n",
    "    ## The only change from SelfAttention and attention is that\n",
    "    ## now we expect 3 sets of encodings to be passed in...\n",
    "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n",
    "        ## ...and we pass those sets of encodings to the various weight matrices.\n",
    "        q = self.W_q(encodings_for_q)\n",
    "        k = self.W_k(encodings_for_k)\n",
    "        v = self.W_v(encodings_for_v)\n",
    "\n",
    "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
    "\n",
    "        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "            \n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76b54e79-4148-49c8-ab15-06ad33eecfd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0100, 1.0641],\n",
       "        [0.2040, 0.7057],\n",
       "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create matrices of token encodings...\n",
    "## cross encodings \n",
    "encodings_for_q = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "encodings_for_k = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "encodings_for_v = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "## set the seed for the random number generator\n",
    "torch.manual_seed(42)\n",
    "\n",
    "## create an attention object\n",
    "attention = Attention(d_model=2,\n",
    "                      row_dim=0,\n",
    "                      col_dim=1)\n",
    "\n",
    "## calculate encoder-decoder attention\n",
    "attention(encodings_for_q, encodings_for_k, encodings_for_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e876720-6cb8-4f4f-bcef-95bad66facf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## multi-head attention \n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 d_model=2,  \n",
    "                 row_dim=0, \n",
    "                 col_dim=1, \n",
    "                 num_heads=1):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        ## create a bunch of attention heads\n",
    "        d_head = d_model//num_heads if d_model >= num_heads and d_model % num_heads == 0 else d_model\n",
    "        self.heads = nn.ModuleList(\n",
    "            [Attention(d_head, row_dim, col_dim) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "        self.col_dim = col_dim\n",
    "\n",
    "        \n",
    "    def forward(self, \n",
    "                encodings_for_q, \n",
    "                encodings_for_k,\n",
    "                encodings_for_v):\n",
    "\n",
    "        ## run the data through all of the attention heads\n",
    "        return torch.cat([head(encodings_for_q, \n",
    "                               encodings_for_k,\n",
    "                               encodings_for_v) \n",
    "                          for head in self.heads], dim=self.col_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6be2dca2-553a-4e89-a7ff-3372508f14d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0100, 1.0641],\n",
       "        [0.2040, 0.7057],\n",
       "        [3.4989, 2.2427]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "encodings_for_q = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "encodings_for_k = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "encodings_for_v = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "\n",
    "\n",
    "## create an attention object\n",
    "multiHeadAttention = MultiHeadAttention(d_model=2,\n",
    "                                        row_dim=0,\n",
    "                                        col_dim=1,\n",
    "                                        num_heads=1)\n",
    "\n",
    "## calculate encoder-decoder attention\n",
    "multiHeadAttention(encodings_for_q, encodings_for_k, encodings_for_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf85884-c837-45f8-b9a8-bed5f779962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the seed for the random number generator\n",
    "torch.manual_seed(42)\n",
    "# output from self attention \n",
    "encodings_for_q = torch.tensor([[1.16],\n",
    "                                [0.57],\n",
    "                                [4.41]])\n",
    "# output from last layer of encoder\n",
    "encodings_for_k = torch.tensor([[1.16],\n",
    "                                [0.57],\n",
    "                                [4.41]])\n",
    "# output from last layer of encoder \n",
    "encodings_for_v = torch.tensor([[1.16],\n",
    "                                [0.57],\n",
    "                                [4.41]])\n",
    "\n",
    "\n",
    "## create an attention object\n",
    "multiHeadAttention = MultiHeadAttention(d_model=2,\n",
    "                                        row_dim=0,\n",
    "                                        col_dim=1,\n",
    "                                        num_heads=2)\n",
    "\n",
    "## calculate encoder-decoder attention\n",
    "result = multiHeadAttention(encodings_for_q, encodings_for_k, encodings_for_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e57d59c-8c87-49de-bf96-87fd2a83acd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93587e7-b96b-4975-9829-46d348453cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the seed for the random number generator\n",
    "torch.manual_seed(42)\n",
    "\n",
    "encodings_for_q = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "encodings_for_k = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "encodings_for_v = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "\n",
    "## create an attention object\n",
    "multiHeadAttention = MultiHeadAttention(d_model=2,\n",
    "                                        row_dim=0,\n",
    "                                        col_dim=1,\n",
    "                                        num_heads=3)\n",
    "\n",
    "## calculate encoder-decoder attention\n",
    "result = multiHeadAttention(encodings_for_q, encodings_for_k, encodings_for_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3554488-d73c-40b3-b419-d84486b2ecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf5ef80-572e-4d12-8077-86765071f7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_back = nn.Linear(in_features=result.size(1), out_features=encodings_for_q.size(1), bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14734c62-f6b7-4589-a38d-819e5b32a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_back(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebookenv",
   "language": "python",
   "name": "notebookenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
